# My First DBT Project

## The Inspiration

This project was inspired by stakeholders raising concerns about perfoming current data tasks in the future state data platform that will utilize DBT for data transformations. While other data engineers on my team exclaimed how straightforward DBT was, as a business analyst, I could understand the fears my stakeholders expressed. Github workflows and utilizing an IDE like VS Code can seem very complex to a non-technical professional.

On a Saturday morning, I woke up and started to peruse the DBT labs website. I created a username and watched some of the training videos. Next, I started the jaffle shop guided exercise. Before starting any modules in the course, I signed into my dormant Github account and downloaded VS Code. In the past, when I tried to use VS Code, all the panels and options overwhelmed me and Iopted for tools such as PyCharm. While  most Github trainings walk through the concept of version control. I was lucky to stumble upon one that incorporated working with directories and the Github workflow. 

For those new to the data space that are trying to feel comfortable with git and Github, I recommend searching for videos and trainings that utilize Github. This way you understand how your local machine works with remote. When you see someone else create a folder/directory on their system and then clone a repo into it, you will have a moment. Then when you see this person create files to the cloned repo's directory, commit and stage these changes before pushing them back to the remote server, all fears about Github will quickly dissipate. Still, I didn't want to take the course until I played around in my terminal window of VS Code. After some quick walkthroughs, I began the Jaffle Shop modules and have been hooked since. 

I originally intended to replicate the tech stack that the data platform being built at my company - AWS SMUS. However, the Jaffle Shop's CloudFront formation JSON had the last generation sizing details for the Redshift cluster - it hadn't been updated for the new serverless flavor of Redshift. I tried a few times to work around this issue, but something just told me this would be a 'nice-to-have' time investment. My goal was to learn DBT - not configuring AWS stuff. I took a Databricks course the previous weekend and heard how awesome Snowflake was. I looked into Snowflake and felt it would be the best solution and a new technology to familiarize myself with.

I am so thankful that I made this decision. Snowflake is an amazing data warehouse. After completing the Jaffle-Shop exercise I felt that I needed to reinforce my learnings with something more hands-on that didn't have training wheels. With all sports betting companies advertisements and personal love of sport, I have longed for some time to injest data from an API for the NFL or NBA so that I could clean it and prep it to be run through some ML model to assist with predicting sports outcomes. Unfortunately, I couldn't find the sort of API I wanted related to sports, but knew there were plenty out there for financial data. I pivoted and decided to complete a data workflow that sought to forecast equity performance. 

Since making that decison, I have learned so much and am constantly amazed at how fast time goes by when working on this project in my personal time. ðŸ˜†ðŸ˜†ðŸ˜†

## Biggest Lessons So Far.....







## Learn From Generative AI..Just Don't Consume it


