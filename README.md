# My First DBT Project

## Table of Contents
- [The Inspiration](#the-inspiration)
- [Biggest Lessons So Far.....](#biggest-lessons-so-far)
- [Learn From Generative AI..Just Don't Consume it](#learn-from-generative-aijust-dont-consume-it)


## The Inspiration

This project was inspired by stakeholders raising concerns about perfoming current data tasks in the future state data platform that will utilize DBT for data transformations. While other data engineers on my team exclaimed how straightforward DBT was, as a business analyst, I could understand the fears my stakeholders expressed. Github workflows and utilizing an IDE like VS Code can seem very complex to a non-technical professional.

On a Saturday morning, I woke up and started to peruse the DBT labs website. I created a username and watched some of the training videos. Next, I started the jaffle shop guided exercise. Before starting any modules in the course, I signed into my dormant Github account and downloaded VS Code. In the past, when I tried to use VS Code, all the panels and options overwhelmed me and Iopted for tools such as PyCharm. While  most Github trainings walk through the concept of version control. I was lucky to stumble upon one that incorporated working with directories and the Github workflow. 

For those new to the data space that are trying to feel comfortable with git and Github, I recommend searching for videos and trainings that utilize Github. This way you understand how your local machine works with remote. When you see someone else create a folder/directory on their system and then clone a repo into it, you will have a moment. Then when you see this person create files to the cloned repo's directory, commit and stage these changes before pushing them back to the remote server, all fears about Github will quickly dissipate. Still, I didn't want to take the course until I played around in my terminal window of VS Code. After some quick walkthroughs, I began the Jaffle Shop modules and have been hooked since. 

I originally intended to replicate the tech stack that the data platform being built at my company - AWS SMUS. However, the Jaffle Shop's CloudFront formation JSON had the last generation sizing details for the Redshift cluster - it hadn't been updated for the new serverless flavor of Redshift. I tried a few times to work around this issue, but something just told me this would be a 'nice-to-have' time investment. My goal was to learn DBT - not configuring AWS stuff. I took a Databricks course the previous weekend and heard how awesome Snowflake was. I looked into Snowflake and felt it would be the best solution and a new technology to familiarize myself with.

I am so thankful that I made this decision. Snowflake is an amazing data warehouse. After completing the Jaffle-Shop exercise I felt that I needed to reinforce my learnings with something more hands-on that didn't have training wheels. With all sports betting companies advertisements and personal love of sport, I have longed for some time to injest data from an API for the NFL or NBA so that I could clean it and prep it to be run through some ML model to assist with predicting sports outcomes. Unfortunately, I couldn't find the sort of API I wanted related to sports, but knew there were plenty out there for financial data. I pivoted and decided to complete a data workflow that sought to forecast equity performance. 

Since making that decison, I have learned so much and am constantly amazed at how fast time goes by when working on this project in my personal time. ðŸ˜†ðŸ˜†ðŸ˜†

## Biggest Lessons So Far.....
1. _Navigating VS Code_: In the past, I used IDEs such as Atom or Jupyter Notebook. Whenever I tried to use VS Code, it alwasys felt intimidating. However, working on this project provided me with the opportunity to familiarize myself with file explorer, the terminal window and the github integration. For those new to 'data', personal projects are the best way to feel comfortable with tools. One important thing to keep in mind is that there are many ways to arrive at the same result. Embrace the method that works for you with tools such as VS Code. Once you are familiar with the tool and comfortable performing tasks that enable your support flow, then you can level up and explore other avenues. If you are collaborating with more experienced developers, take note of best practices and try to incorporate the gems they share with you. However, when you start to feel overloaded, take that time out and breath before continuing.
2. _Git Workflow_: Many tutorials on Git usually demo this version control technology with a single file. This brand of Git introduction is decent to learn the Git workflow. However, for most projects, you are working with directories - which is why the file explorer feature in VS Code is so powerful. Secondly, when working on projects with others, you often will have to initiate pull requests so your work can be incorporated into the project's corpus. The DBT Jaffle Shop guided project offers a great opportunity to familiarize yourself with the Git workflow. As you begin to update files, remind yourself to commit for each atomic change. By this, if you modifying three configurationn files, commit after each update. This helps you revert back to prior versions and also helps when you have to create a pull request.
3. _DBT Init_: After completing the guided Jaffle Shop project, I spent a Saturday morning trying to set up this stock market exercise. This was largely due to not appreciating the full power of DBT commands such as DBT Init. I didn't fully understand how this command creates your basic directory structure for your project that you can then refine as needed.  For those exploring DBT for the first time, take time to appreciate the various configuration ".yml" files such as your system-level profile.yml and your dbt_project.yml. Many errors you will encounter usually involve these configuration items. While you may not initially enjoy spinning your wheels for an hour or two, the lessons learned will pay huge dividends as you advance in your data journey.
4. _Snowflake is Awesome_: I don't have any real formal experience with other data warehouse platforms outside of the products offered by AWS. I took a Databricks intro course, but never really engaged the platform. With this limited exposure, please don't consider me any sort of authority of data platforms, but I have found Snowflake to be very straightforward and simple to set up and configure. Its SQL-native approach couples well with DBT's similar SQL approach. I am of an age where MS-DOS was prominent and I rarely touched a computer because I didn't really get the whole prompt concept at 7/8 years old. When teachers used Macs in school and we finally got one at home, I felt at ease doing the same things on the Mac that my grandfather and other relatives tried to show my on the PC. This is how I feel about Snowflake - it is the Mac and AWS' products feel like that DOS system. Snowflake is designed for data where AWS is optimal for hosting applications, but can do data functions as well. Now that I have better footing with DBT, I will probably add the requisite AWS connections to my dbt_profile to do a project where AWS is my data warehouse.
5. _System Variables_: As my project developed and I wanted to schedule jobs to run my daily API scripts, having my AWS credentials and API keys hardcoded in my scripts wasn't a security best practice. At work, I heard my data engineering colleagues mention system variables, but I didn't fully understand their use cases. ChatGPT actually recommended utilizing these variables when I was refining a script and the AI called out the security risk of storing such details on Github. After executing some commands in my terminal and declaring variables on Github, I was able to set up Actions to run the script daily via Github. I am considering executing these daily jobs via a AWS Lambda function call to leverage EC2 computing power. 
   
## Learn From Generative AI..Just Don't Consume it
I utilized Generative AI tools such as Claude, Perplexity and ChatGPT for this project. My primary use case was assisting me with the python scripts to make get requests from the AlphaVantange API to obtain to stock data. Without AI, I probably could have created these scripts over the course of three sessions and the product would have been not as efficient. While Perplexity allowed me to run these scripts after an hour or two, I still reviewed them to understand the python libraries used and take note of code's syntax. With anything code-related, your best learning opportunities come when errors are thrown. Even if you do not know the solution to employ to resolve these errors, if you can identify the root cause and pinpoint it to your codebase, you are slowly becoming proficient. Like the patterns you notice in the code blocks generated, with certain errors you will notice common strategies to resolving errors.

Without AI, I might have been able to complete this project, but it would have taken longer. I encourage those new to the data space to leverage this tool as well, but just do not consume the outputs it generates. When I engage with ChatGPT or Claude, I now state what I want to accomplish and then provide what I think are the steps to accomplish that goal. It is reassuring when the AI responds with that is a correct approach and here are some better ways to develop your project. Even when the response is referring you to get tested for illicit substances, you are learning and starting to think "programatically". This approach really helped me become comfortable with the Git workflow. For example, if I needed to pull down the latest branch from my remote server and merge with my local branch, I would state my objective and then ask if executing a git pull is correct. 

Do not just create prompts to get solutions. Instead, engage in conversations with your AI tool as you would in a classroom session.


